{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic_settings import BaseSettings\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import operator\n",
    "from typing import  Annotated\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sqlalchemy.sql import text\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List, TypedDict, Dict, Any, Optional\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import HumanMessage,  SystemMessage\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import create_engine, Column, Integer, Float, String, MetaData, Table, text, inspect\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for uploading the csv file to a database and also connecting to postgres database using sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Settings(BaseSettings):\n",
    "    database_hostname: str\n",
    "    database_port: str\n",
    "    database_password: str\n",
    "    database_name: str\n",
    "    database_username: str\n",
    "\n",
    "    openai_api_key: str\n",
    "    langsmith_tracing: bool\n",
    "    langsmith_endpoint: str\n",
    "    langsmith_api_key: str\n",
    "    langsmith_project: str\n",
    "    tavily_api_key: str\n",
    "    upload_folder: str\n",
    "    class Config:\n",
    "        env_file = \".env\"\n",
    "        extra = \"allow\"\n",
    "\n",
    "\n",
    "settings = Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create state for passing data between the nodes for the database creation\n",
    "class GenerateAnalystsState(TypedDict):\n",
    "    csv_path: str \n",
    "    analysis_goal: str  \n",
    "    max_analysts: int\n",
    "    human_analyst_feedback: Optional[str]\n",
    "    data_database_name: Optional[str] \n",
    "    memory_database_name: Optional[str]\n",
    "    table_name: Optional[str]  \n",
    "    data_df: Dict[str, Any] \n",
    "    ml_models: Dict[str, Any]\n",
    "    visualizations: Dict[str, str] \n",
    "    database_schema: Optional[Dict[str, str]] \n",
    "    analysis_summary: Optional[str]\n",
    "    memory_saver: str\n",
    "    data_engine: str\n",
    "    question: str\n",
    "    sql_query: str\n",
    "    query_result: str\n",
    "    query_rows : str\n",
    "    sql_error: bool\n",
    "    attempts: int\n",
    "    relevance: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_column_types(df):\n",
    "    \"\"\"Infer SQL column types from csv file\"\"\"\n",
    "    type_map = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_integer_dtype(df[col]):\n",
    "            type_map[col] = Integer\n",
    "        elif pd.api.types.is_float_dtype(df[col]):\n",
    "            type_map[col] = Float\n",
    "        else:\n",
    "            type_map[col] = String(255)\n",
    "    \n",
    "    return type_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_databases_and_tables(state: GenerateAnalystsState, df, data_db_name=None, memory_db_name=None, table_name=None):\n",
    "    \"\"\"Create two databases: one for storing CSV data and another for memory\"\"\"\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    if data_db_name is None:\n",
    "        data_db_name = settings.database_name\n",
    "        if data_db_name is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            data_db_name = f\"analysis_db_{timestamp}\"\n",
    "    if memory_db_name is None:\n",
    "        memory_db_name = \"memory_db\"\n",
    "\n",
    "    if table_name is None:\n",
    "        table_name = \"data_table\"\n",
    "\n",
    "    settings.database_name = data_db_name\n",
    "\n",
    "    admin_engine = create_engine(\n",
    "        f\"postgresql+psycopg2://{settings.database_username}:{settings.database_password}@\"\n",
    "        f\"{settings.database_hostname}:{settings.database_port}/postgres\"\n",
    "    )\n",
    "\n",
    "    with admin_engine.connect() as conn:\n",
    "        conn.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
    "        \n",
    "        conn.execute(text(f\"SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = '{data_db_name}'\"))\n",
    "        conn.execute(text(f\"DROP DATABASE IF EXISTS {data_db_name}\"))\n",
    "        conn.execute(text(f\"CREATE DATABASE {data_db_name}\"))\n",
    "\n",
    "        conn.execute(text(f\"SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = '{memory_db_name}'\"))\n",
    "        conn.execute(text(f\"DROP DATABASE IF EXISTS {memory_db_name}\"))\n",
    "        conn.execute(text(f\"CREATE DATABASE {memory_db_name}\"))\n",
    "\n",
    "    # Connect to data database\n",
    "    data_engine = create_engine(\n",
    "        f\"postgresql+psycopg2://{settings.database_username}:{settings.database_password}@\"\n",
    "        f\"{settings.database_hostname}:{settings.database_port}/{data_db_name}\"\n",
    "    )\n",
    "\n",
    "    # Connect to memory database\n",
    "    memory_engine = create_engine(\n",
    "        f\"postgresql+psycopg2://{settings.database_username}:{settings.database_password}@\"\n",
    "        f\"{settings.database_hostname}:{settings.database_port}/{memory_db_name}\"\n",
    "    )\n",
    "\n",
    "    # Create table for CSV data\n",
    "    metadata = MetaData()\n",
    "    columns = [Column('id', Integer, primary_key=True)]\n",
    "    schema_info = {'id': 'Integer (primary key)'}\n",
    "\n",
    "    column_types = infer_column_types(df)\n",
    "    for col_name, sql_type in column_types.items():\n",
    "        safe_col_name = col_name.replace(' ', '_').lower()\n",
    "        columns.append(Column(safe_col_name, sql_type))\n",
    "        schema_info[safe_col_name] = sql_type.__name__ if isinstance(sql_type, type) else str(sql_type)\n",
    "\n",
    "    table = Table(table_name, metadata, *columns)\n",
    "    metadata.create_all(data_engine)\n",
    "\n",
    "    df_to_insert = df.copy()\n",
    "    df_to_insert.columns = [col.replace(' ', '_').lower() for col in df.columns]\n",
    "\n",
    "    with data_engine.connect() as conn:\n",
    "        batch_size = 1000\n",
    "        for i in range(0, len(df_to_insert), batch_size):\n",
    "            batch = df_to_insert.iloc[i:i+batch_size]\n",
    "            batch.to_sql(table_name, conn, if_exists='append', index=False)\n",
    "\n",
    "    # Create table for memory storage\n",
    "    memory_metadata = MetaData()\n",
    "    memory_table = Table(\n",
    "        \"memory_store\", memory_metadata,\n",
    "        Column(\"id\", Integer, primary_key=True),\n",
    "        Column(\"session_id\", String, nullable=False),\n",
    "        Column(\"state\", String, nullable=False)\n",
    "    )\n",
    "    memory_metadata.create_all(memory_engine)\n",
    "\n",
    "    \n",
    "    memory_saver = PostgresSaver(f\"postgresql://{settings.database_username}:{settings.database_password}@\"\n",
    "                                 f\"{settings.database_hostname}:{settings.database_port}/{memory_db_name}\")\n",
    "    \n",
    "    state[\"data_database_name\"] = data_db_name\n",
    "\n",
    "    return {\n",
    "        \"data_database_name\": data_db_name,\n",
    "        \"memory_database_name\": memory_db_name,\n",
    "        \"table_name\": table_name,\n",
    "        \"data_engine\": data_engine,\n",
    "        \"memory_engine\": memory_engine,\n",
    "        \"schema\": schema_info,\n",
    "        \"memory_saver\": memory_saver\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_and_create_db(state: GenerateAnalystsState):\n",
    "    \"\"\"Load CSV and create database\"\"\"\n",
    "    csv_path = state['csv_path']\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        db_info = create_databases_and_tables(\n",
    "            state,\n",
    "            df, \n",
    "            data_db_name=state.get('data_database_name'),\n",
    "            table_name=state.get('table_name')\n",
    "        )\n",
    "        \n",
    "        state[\"data_df\"] = {\"raw\": df.to_dict(orient=\"records\")}\n",
    "        state[\"data_database_name\"] = db_info['data_database_name']\n",
    "        state[\"table_name\"] =  db_info['table_name'],\n",
    "        state[\"database_schema\"] = db_info['schema'], \n",
    "        state[\"data_engine\"] = db_info['data_engine'],\n",
    "        state[\"memory_engine\"] =  db_info['memory_engine'],\n",
    "        state[\"schema\"] =  db_info['schema'],\n",
    "        state[\"memory_saver\"] = db_info['memory_saver']\n",
    "        \n",
    "        return {\n",
    "            'data_df': state[\"data_df\"],\n",
    "            \"data_database_name\": db_info['data_database_name'],\n",
    "            \"table_name\": db_info['table_name'],\n",
    "            \"database_schema\": db_info['schema'], \n",
    "            \"data_engine\": db_info['data_engine'],\n",
    "            \"memory_engine\": db_info['memory_engine'],\n",
    "            \"schema\": db_info['schema'],\n",
    "            \"memory_saver\": db_info['memory_saver']\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"data_df\": {\"error\": str(e)},\n",
    "            \"error_message\": f\"Failed to process CSV and create database: {str(e)}\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_connection(state: GenerateAnalystsState):\n",
    "    \"\"\"Get a database connection to the specified database\"\"\"\n",
    "    db_name = state['data_database_name']\n",
    "    \n",
    "    if not db_name:\n",
    "        raise ValueError(\"No database name specified\")\n",
    "    \n",
    "    engine = create_engine(\n",
    "        f\"postgresql+psycopg2://{settings.database_username}:{settings.database_password}@\"\n",
    "        f\"{settings.database_hostname}:{settings.database_port}/{db_name}\"\n",
    "    )\n",
    "    \n",
    "    SessionLocal = sessionmaker(autoflush=False, bind=engine, autocommit=False)\n",
    "    db = SessionLocal()\n",
    "    \n",
    "    return db, engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent for Quering the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckRelevance(BaseModel):\n",
    "    relevance: str = Field(\n",
    "        description=\"Indicates whether the question is related to the database schema. 'relevant' or 'not_relevant'.\"\n",
    "    )\n",
    "\n",
    "def check_relevance(state: GenerateAnalystsState, config: RunnableConfig):\n",
    "    question = state[\"question\"]\n",
    "    schema = state[\"database_schema\"]\n",
    "    print(f\"Checking relevance of the question: {question}\")\n",
    "    system = \"\"\"You are an assistant that determines whether a given question is related to the following database schema.\n",
    "\n",
    "Schema:\n",
    "{schema}\n",
    "\n",
    "Respond with only \"relevant\" or \"not_relevant\".\n",
    "\"\"\".format(schema=schema)\n",
    "    human = f\"Question: {question}\"\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    structured_llm = llm.with_structured_output(CheckRelevance)\n",
    "    \n",
    "    relevance = structured_llm.invoke([SystemMessage(content=system)]+[HumanMessage(content=human)])\n",
    "    \n",
    "    state[\"relevance\"] = relevance.relevance\n",
    "    print(f\"Relevance determined: {state['relevance']}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToSQL(BaseModel):\n",
    "    sql_query: str = Field(\n",
    "        description=\"The SQL query corresponding to the user's natural language question.\"\n",
    "    )\n",
    "\n",
    "def convert_nl_to_sql(state: GenerateAnalystsState, config: RunnableConfig):\n",
    "    question = state[\"question\"]\n",
    "    schema = state[\"database_schema\"]\n",
    "    table_name = state.get(\"table_name\", \"your_table_name\") \n",
    "    \n",
    "    print(f\"Converting question to SQL for user: {question}\")\n",
    "    \n",
    "    system = \"\"\"You are an assistant that converts natural language questions into SQL queries based on the following schema:\n",
    "\n",
    "{schema}\n",
    "\n",
    "- The table name is: {table_name}\n",
    "- Always use the table name '{table_name}' in your SQL queries.\n",
    "- Provide only the SQL query without any explanations.\n",
    "- Alias columns appropriately to match the expected keys in the result.\n",
    "\"\"\".format(schema=schema, table_name=table_name)\n",
    "    \n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(ConvertToSQL)\n",
    "    \n",
    "    result = structured_llm.invoke([SystemMessage(content=system)] + [HumanMessage(content=question)])\n",
    "    \n",
    "    state[\"sql_query\"] = result.sql_query\n",
    "    print(f\"Generated SQL query: {state['sql_query']}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql(state: GenerateAnalystsState):\n",
    "    sql_query = state[\"sql_query\"].strip()\n",
    "    table_name = state[\"table_name\"]\n",
    "    sql_query = sql_query.replace(\"your_table_name\", table_name)\n",
    "    \n",
    "    session, engine = get_db_connection(state)\n",
    "    print(f\"Executing SQL query: {sql_query}\")\n",
    "    try:\n",
    "        result = session.execute(text(sql_query))\n",
    "        if sql_query.lower().startswith(\"select\"):\n",
    "            rows = result.fetchall()\n",
    "            columns = result.keys()\n",
    "            if rows:\n",
    "                # Store the raw rows as a list of dictionaries\n",
    "                state[\"query_rows\"] = [dict(zip(columns, row)) for row in rows]\n",
    "                print(f\"Raw SQL Query Result: {state['query_rows']}\")\n",
    "                # Format the result as a simple string representation\n",
    "                formatted_result = \"\\n\".join([str(row) for row in state[\"query_rows\"]])\n",
    "            else:\n",
    "                state[\"query_rows\"] = []\n",
    "                formatted_result = \"No results found.\"\n",
    "            state[\"query_result\"] = formatted_result\n",
    "            state[\"sql_error\"] = False\n",
    "            print(\"SQL SELECT query executed successfully.\")\n",
    "        else:\n",
    "            session.commit()\n",
    "            state[\"query_result\"] = \"The action has been successfully completed.\"\n",
    "            state[\"sql_error\"] = False\n",
    "            print(\"SQL command executed successfully.\")\n",
    "    except Exception as e:\n",
    "        state[\"query_result\"] = f\"Error executing SQL query: {str(e)}\"\n",
    "        state[\"sql_error\"] = True\n",
    "        print(f\"Error executing SQL query: {str(e)}\")\n",
    "    finally:\n",
    "        session.close()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_human_readable_answer(state: GenerateAnalystsState):\n",
    "    sql = state[\"sql_query\"]\n",
    "    result = state[\"query_result\"]\n",
    "    query_rows = state.get(\"query_rows\", [])\n",
    "    sql_error = state.get(\"sql_error\", False)\n",
    "    schema = state[\"database_schema\"]\n",
    "    print(\"Generating a human-readable answer.\")\n",
    "\n",
    "    system = f\"\"\"You are an AI assistant that converts SQL query results into clear, natural language responses for any database.\n",
    "- Your responses should be structured, concise, and friendly.\n",
    "- Ensure clarity by avoiding technical jargon.\n",
    "- If there is an error, return a user-friendly explanation.\n",
    "- If there are no results, state that clearly.\n",
    "- Always greet the user using their name.\n",
    "\n",
    "Database Schema:\n",
    "{schema}\n",
    "\"\"\"\n",
    "\n",
    "    structured_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "    if sql_error:\n",
    "        # Handle SQL errors\n",
    "        messages = [\n",
    "            SystemMessage(content=system),\n",
    "            HumanMessage(\n",
    "                content=f\"\"\"SQL Query:\n",
    "{sql}\n",
    "\n",
    "Error:\n",
    "{result}\n",
    "\n",
    "Please explain the error in simple terms and suggest a potential solution.\"\"\"\n",
    "            ),\n",
    "        ]\n",
    "    elif sql.lower().startswith(\"select\"):\n",
    "        if not query_rows:\n",
    "            # Handle empty query results\n",
    "            messages = [\n",
    "                SystemMessage(content=system),\n",
    "                HumanMessage(\n",
    "                    content=f\"\"\"SQL Query:\n",
    "{sql}\n",
    "\n",
    "Result:\n",
    "{result}\n",
    "\n",
    "Convert this SQL result into a natural language response. Start with 'Hello,' and indicate that no matching records were found.\"\"\"\n",
    "                ),\n",
    "            ]\n",
    "        else:\n",
    "            # Handle successful query results\n",
    "            messages = [\n",
    "                SystemMessage(content=system),\n",
    "                HumanMessage(\n",
    "                    content=f\"\"\"SQL Query:\n",
    "{sql}\n",
    "\n",
    "Result:\n",
    "{result}\n",
    "\n",
    "Convert this SQL result into a human-readable summary. Start with 'Hello ,' and provide a clear summary of the retrieved records.\"\"\"\n",
    "                ),\n",
    "            ]\n",
    "    else:\n",
    "        # Handle non-SELECT queries (INSERT, UPDATE, DELETE, etc.)\n",
    "        messages = [\n",
    "            SystemMessage(content=system),\n",
    "            HumanMessage(\n",
    "                content=f\"\"\"SQL Query:\n",
    "{sql}\n",
    "\n",
    "Result:\n",
    "{result}\n",
    "\n",
    "Confirm the action performed in a simple, natural language format. Start with 'Hello,' and clearly state what was updated, inserted, or deleted.\"\"\"\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    human_response = structured_llm.invoke(messages)\n",
    "    state[\"query_result\"] = human_response\n",
    "    print(\"Generated human-readable answer.\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewrittenQuestion(BaseModel):\n",
    "    question: str = Field(description=\"The rewritten question.\")\n",
    "\n",
    "def regenerate_query(state: GenerateAnalystsState):\n",
    "    question = state[\"question\"]\n",
    "    print(\"Regenerating the SQL query by rewriting the question.\")\n",
    "\n",
    "    system = \"\"\"You are an AI assistant that reformulates user questions to improve SQL query precision. \n",
    "- Ensure all required details are retained.\n",
    "- Optimize the question for structured data retrieval.\n",
    "- Maintain accuracy, including table joins where necessary.\"\"\"\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    structured_llm = llm.with_structured_output(RewrittenQuestion)\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=system),\n",
    "        HumanMessage(\n",
    "            content=f\"\"\"Original Question: {question}\n",
    "\n",
    "Please rewrite this question to make it clearer and more precise for generating a SQL query while preserving necessary details.\"\"\"\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    rewritten = structured_llm.invoke(messages)\n",
    "    state[\"question\"] = rewritten.question\n",
    "    state[\"attempts\"] += 1\n",
    "    print(f\"Rewritten question: {state['question']}\")\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "tavily_search = TavilySearchResults(max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchQuery(BaseModel):\n",
    "    search_query: str = Field(None, description=\"Search query for retrieval.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search query writing\n",
    "search_instructions = SystemMessage(content=f\"\"\"\n",
    "\"You are an intelligent and efficient information retrieval agent. Your primary task is to assist users by providing accurate and relevant information. \n",
    "\n",
    "\n",
    "Please state to the information you requested is not available in the database. I will now search the web to find the most accurate and up-to-date information for you.\n",
    "\n",
    "Search the Web:  \n",
    "   - Use a web search tool to find reliable and relevant information from credible sources.  \n",
    "   - Prioritize authoritative websites, recent data, and trustworthy publications.  \n",
    "\n",
    "Summarize and Respond:  \n",
    "   - Extract key details from the search results and provide a concise, clear, and accurate summary.  \n",
    "   - Include only the most relevant information to answer the user's query.  \n",
    "\n",
    "Cite Sources:  \n",
    "   - Always include the source(s) of your information (e.g., website URLs or publication names) to ensure transparency and allow the user to verify the information.  \n",
    "\n",
    "Format the Response:  \n",
    "   - Present your response in a structured and easy-to-read format. Use bullet points, numbered lists, or paragraphs as appropriate.  \n",
    "\n",
    "Handle Ambiguity:  \n",
    "   - If the query is unclear or lacks sufficient detail, ask follow-up questions to clarify the user's intent before proceeding.  \n",
    "\n",
    "Stay Neutral and Objective:  \n",
    "   - Avoid personal opinions or biases. Provide factual information based on the database or web search results.  \n",
    "\n",
    "Example Workflow:  \n",
    "\n",
    "User Query: *What is the capital of France?*   \n",
    "- The capital of France is Paris.  \n",
    "- Source: [CIA World Factbook - France](https://www.cia.gov/the-world-factbook/countries/france/)  \n",
    "\n",
    "\"\"\")\n",
    "\n",
    "def search_web(state: GenerateAnalystsState):\n",
    "    \n",
    "    \"\"\" Retrieve docs from web search \"\"\"\n",
    "\n",
    "    # Search query\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query = structured_llm.invoke([search_instructions]+state['query'])\n",
    "    \n",
    "    # Search\n",
    "    search_docs = tavily_search.invoke(search_query.search_query)\n",
    "\n",
    "     # Format\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"query_result\": [formatted_search_docs]} \n",
    "\n",
    "def search_wikipedia(state: GenerateAnalystsState):\n",
    "    \n",
    "    \"\"\" Retrieve docs from wikipedia \"\"\"\n",
    "\n",
    "    # Search query\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query = structured_llm.invoke([search_instructions]+ state['query'])\n",
    "    \n",
    "    # Search\n",
    "    search_docs = WikipediaLoader(query=search_query.search_query, \n",
    "                                  load_max_docs=2).load()\n",
    "\n",
    "     # Format\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document source=\"{doc.metadata[\"source\"]}\" page=\"{doc.metadata.get(\"page\", \"\")}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"query_result\": [formatted_search_docs]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_funny_response(state: GenerateAnalystsState):\n",
    "    print(\"Generating a funny response for an unrelated question.\")\n",
    "\n",
    "    system = \"\"\"You are a charming and humorous assistant who responds in a playful and witty manner.\n",
    "Have fun with your response while keeping it lighthearted!\"\"\"\n",
    "\n",
    "    human = \"\"\"I may not have the answer to that, but let's not take life too seriously! \n",
    "How about a fun fact instead? Did you know that octopuses have three hearts?\"\"\"\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=system),\n",
    "        HumanMessage(content=human),\n",
    "    ]\n",
    "\n",
    "    funny_response = llm.invoke(messages)\n",
    "    state[\"query_result\"] = funny_response.content\n",
    "\n",
    "    print(\"Generated funny response.\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_max_iterations(state: GenerateAnalystsState):\n",
    "    state[\"query_result\"] = \"Please try again.\"\n",
    "    print(\"Maximum attempts reached. Ending the workflow.\")\n",
    "    return state\n",
    "\n",
    "def relevance_router(state: GenerateAnalystsState):\n",
    "    if state[\"relevance\"].lower() == \"relevant\":\n",
    "        return \"convert_to_sql\"\n",
    "    else:\n",
    "        return \"generate_funny_response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_attempts_router(state: GenerateAnalystsState):\n",
    "    if state[\"attempts\"] < 3:\n",
    "        return \"convert_to_sql\"\n",
    "    else:\n",
    "        return \"end_max_iterations\"\n",
    "\n",
    "def execute_sql_router(state: GenerateAnalystsState):\n",
    "    if not state.get(\"sql_error\", False):\n",
    "        return \"search_web\"\n",
    "    else:\n",
    "        return \"regenerate_query\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph( GenerateAnalystsState)\n",
    "\n",
    "builder.add_node(\"process_csv_and_create_db\", process_csv_and_create_db)\n",
    "builder.add_node(\"check_relevance\", check_relevance)\n",
    "builder.add_node(\"convert_to_sql\", convert_nl_to_sql)\n",
    "builder.add_node(\"execute_sql\", execute_sql)\n",
    "builder.add_node(\"generate_human_readable_answer\", generate_human_readable_answer)\n",
    "builder.add_node(\"regenerate_query\", regenerate_query)\n",
    "#builder.add_node(\"generate_funny_response\", generate_funny_response)\n",
    "builder.add_node(\"end_max_iterations\", end_max_iterations)\n",
    "builder.add_node(\"search_web\", search_web)\n",
    "#builder.add_edge(\"search_wikipedia\", search_wikipedia)\n",
    "\n",
    "\n",
    "builder.add_edge(\"process_csv_and_create_db\", \"check_relevance\")\n",
    "builder.add_conditional_edges(\n",
    "    \"check_relevance\",\n",
    "    relevance_router,\n",
    "    {\n",
    "        \"convert_to_sql\": \"convert_to_sql\",\n",
    "        \"search_web\": \"search_web\",\n",
    "    },\n",
    ")\n",
    "\n",
    "builder.add_edge(\"convert_to_sql\", \"execute_sql\")\n",
    "\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"execute_sql\",\n",
    "    execute_sql_router,\n",
    "    {\n",
    "        \"generate_human_readable_answer\": \"generate_human_readable_answer\",\n",
    "        \"regenerate_query\": \"regenerate_query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"regenerate_query\",\n",
    "    check_attempts_router,\n",
    "    {\n",
    "        \"convert_to_sql\": \"convert_to_sql\",\n",
    "        \"max_iterations\": \"end_max_iterations\",\n",
    "    },\n",
    ")\n",
    "\n",
    "builder.add_edge(\"generate_human_readable_answer\", END)\n",
    "builder.add_edge(\"search_web\", END)\n",
    "builder.add_edge(\"end_max_iterations\", END)\n",
    "\n",
    "builder.set_entry_point(\"process_csv_and_create_db\")\n",
    "\n",
    "memory = GenerateAnalystsState[\"memory_saver\"]\n",
    "\n",
    "app = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ReadTimeout",
     "evalue": "HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/Data_analyst_agent/agent/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Data_analyst_agent/agent/lib/python3.12/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.12/http/client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1427\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1428\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m/usr/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n",
      "File \u001b[0;32m/usr/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m/usr/lib/python3.12/ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.12/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/Data_analyst_agent/agent/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Data_analyst_agent/agent/lib/python3.12/site-packages/urllib3/connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/Data_analyst_agent/agent/lib/python3.12/site-packages/urllib3/util/retry.py:474\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[0;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Data_analyst_agent/agent/lib/python3.12/site-packages/urllib3/util/util.py:39\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Data_analyst_agent/agent/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/Data_analyst_agent/agent/lib/python3.12/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/Data_analyst_agent/agent/lib/python3.12/site-packages/urllib3/connectionpool.py:367\u001b[0m, in \u001b[0;36mHTTPConnectionPool._raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    369\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m----> 2\u001b[0m display(Image(\u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxray\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[0;32m~/Data_analyst_agent/agent/lib/python3.12/site-packages/langchain_core/runnables/graph.py:624\u001b[0m, in \u001b[0;36mGraph.draw_mermaid_png\u001b[0;34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m draw_mermaid_png\n\u001b[1;32m    619\u001b[0m mermaid_syntax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraw_mermaid(\n\u001b[1;32m    620\u001b[0m     curve_style\u001b[38;5;241m=\u001b[39mcurve_style,\n\u001b[1;32m    621\u001b[0m     node_colors\u001b[38;5;241m=\u001b[39mnode_colors,\n\u001b[1;32m    622\u001b[0m     wrap_label_n_words\u001b[38;5;241m=\u001b[39mwrap_label_n_words,\n\u001b[1;32m    623\u001b[0m )\n\u001b[0;32m--> 624\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Data_analyst_agent/agent/lib/python3.12/site-packages/langchain_core/runnables/graph_mermaid.py:214\u001b[0m, in \u001b[0;36mdraw_mermaid_png\u001b[0;34m(mermaid_syntax, output_file_path, draw_method, background_color, padding)\u001b[0m\n\u001b[1;32m    208\u001b[0m     img_bytes \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    209\u001b[0m         _render_mermaid_using_pyppeteer(\n\u001b[1;32m    210\u001b[0m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     )\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m draw_method \u001b[38;5;241m==\u001b[39m MermaidDrawMethod\u001b[38;5;241m.\u001b[39mAPI:\n\u001b[0;32m--> 214\u001b[0m     img_bytes \u001b[38;5;241m=\u001b[39m \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground_color\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     supported_methods \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([m\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "File \u001b[0;32m~/Data_analyst_agent/agent/lib/python3.12/site-packages/langchain_core/runnables/graph_mermaid.py:336\u001b[0m, in \u001b[0;36m_render_mermaid_using_api\u001b[0;34m(mermaid_syntax, output_file_path, background_color, file_type)\u001b[0m\n\u001b[1;32m    330\u001b[0m         background_color \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackground_color\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    332\u001b[0m image_url \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://mermaid.ink/img/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmermaid_syntax_encoded\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?type=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&bgColor=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackground_color\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    335\u001b[0m )\n\u001b[0;32m--> 336\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    338\u001b[0m     img_bytes \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/Data_analyst_agent/agent/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Data_analyst_agent/agent/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Data_analyst_agent/agent/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Data_analyst_agent/agent/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/Data_analyst_agent/agent/lib/python3.12/site-packages/requests/adapters.py:713\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[0;32m--> 713\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidHeader(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mReadTimeout\u001b[0m: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "display(Image(app.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking relevance of the question: what are the columns in the dataset\n",
      "Relevance determined: relevant\n",
      "Converting question to SQL for user: what are the columns in the dataset\n",
      "Generated SQL query: SELECT column_name FROM information_schema.columns WHERE table_name = 'data_table';\n",
      "Executing SQL query: SELECT column_name FROM information_schema.columns WHERE table_name = 'data_table';\n",
      "Raw SQL Query Result: [{'column_name': 'id'}, {'column_name': 'passengerid'}, {'column_name': 'survived'}, {'column_name': 'pclass'}, {'column_name': 'age'}, {'column_name': 'sibsp'}, {'column_name': 'parch'}, {'column_name': 'fare'}, {'column_name': 'cabin'}, {'column_name': 'name'}, {'column_name': 'sex'}, {'column_name': 'ticket'}, {'column_name': 'embarked'}]\n",
      "SQL SELECT query executed successfully.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'search_web'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 26\u001b[0m\n\u001b[1;32m      1\u001b[0m user_question_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat are the columns in the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m initial_state \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m \t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \n\u001b[1;32m      4\u001b[0m \t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_question_1,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelevance\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m }\n\u001b[0;32m---> 26\u001b[0m result_1 \u001b[38;5;241m=\u001b[39m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m query_result \u001b[38;5;241m=\u001b[39m result_1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_result\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Extract only the \"content\" tuple\u001b[39;00m\n",
      "File \u001b[0;32m~/Data_analyst_agent/agent/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1961\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1959\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1960\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1961\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1962\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1965\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1966\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1967\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1969\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1970\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1971\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m~/Data_analyst_agent/agent/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1670\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1664\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1665\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1667\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1668\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[1;32m   1669\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1670\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[1;32m   1677\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/Data_analyst_agent/agent/lib/python3.12/site-packages/langgraph/pregel/runner.py:171\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    169\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Data_analyst_agent/agent/lib/python3.12/site-packages/langgraph/pregel/retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m~/Data_analyst_agent/agent/lib/python3.12/site-packages/langgraph/utils/runnable.py:450\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    449\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 450\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Data_analyst_agent/agent/lib/python3.12/site-packages/langgraph/utils/runnable.py:219\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 219\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Data_analyst_agent/agent/lib/python3.12/site-packages/langgraph/graph/graph.py:96\u001b[0m, in \u001b[0;36mBranch._route\u001b[0;34m(self, input, config, reader, writer)\u001b[0m\n\u001b[1;32m     94\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m     95\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minvoke(value, config)\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Data_analyst_agent/agent/lib/python3.12/site-packages/langgraph/graph/graph.py:132\u001b[0m, in \u001b[0;36mBranch._finish\u001b[0;34m(self, writer, input, result, config)\u001b[0m\n\u001b[1;32m    129\u001b[0m     result \u001b[38;5;241m=\u001b[39m [result]\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mends:\n\u001b[1;32m    131\u001b[0m     destinations: Sequence[Union[Send, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 132\u001b[0m         r \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(r, Send) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mends\u001b[49m\u001b[43m[\u001b[49m\u001b[43mr\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result\n\u001b[1;32m    133\u001b[0m     ]\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     destinations \u001b[38;5;241m=\u001b[39m cast(Sequence[Union[Send, \u001b[38;5;28mstr\u001b[39m]], result)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'search_web'",
      "\u001b[0mDuring task with name 'execute_sql' and id '09172f61-a4c6-f79b-fe20-13ff78463dc3'"
     ]
    }
   ],
   "source": [
    "user_question_1 = \"what are the columns in the dataset\"\n",
    "initial_state = {\n",
    "\t\"csv_path\": \"train.csv\",  \n",
    "\t\"question\": user_question_1,\n",
    "\t\"attempts\": 0,\n",
    "\t\"data_database_name\": settings.database_name,\n",
    "\t\"memory_database_name\": \"memory_db\",\n",
    "\t\"table_name\": \"data_table\",\n",
    "\t\"sql_error\": False,\n",
    "\t\"analysis_goal\": \"\",\n",
    "\t\"max_analysts\": 3,\n",
    "\t\"human_analyst_feedback\": None,\n",
    "\t\"data_df\": {},\n",
    "\t\"ml_models\": {},\n",
    "\t\"visualizations\": {},\n",
    "\t\"database_schema\": None,\n",
    "\t\"analysis_summary\": None,\n",
    "\t\"memory_saver\": None,\n",
    "\t\"data_engine\": None,\n",
    "\t\"sql_query\": \"\",\n",
    "\t\"query_result\": \"\",\n",
    "\t\"query_rows\": \"\",\n",
    "\t\"relevance\": \"\"\n",
    "}\n",
    "\n",
    "result_1 = app.invoke(initial_state)\n",
    "\n",
    "query_result = result_1[\"query_result\"]\n",
    "\n",
    "# Extract only the \"content\" tuple\n",
    "content_tuple = next((item for item in query_result if item[0] == \"content\"), None)\n",
    "\n",
    "# Get the content text\n",
    "content_text = content_tuple[1] if content_tuple else \"No content found.\"\n",
    "\n",
    "formatted_output = f\"\\n{content_text}\"\n",
    "\n",
    "print(formatted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking relevance of the question: how many passengers survived\n",
      "Relevance determined: relevant\n",
      "Converting question to SQL for user: how many passengers survived\n",
      "Generated SQL query: SELECT COUNT(*) AS survived_count FROM data_table WHERE survived = 1;\n",
      "Executing SQL query: SELECT COUNT(*) AS survived_count FROM data_table WHERE survived = 1;\n",
      "Raw SQL Query Result: [{'survived_count': 342}]\n",
      "SQL SELECT query executed successfully.\n",
      "Generating a human-readable answer.\n",
      "Generated human-readable answer.\n",
      "Result: content='Hello, \\n\\nA total of 342 passengers survived.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 242, 'total_tokens': 254, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None} id='run-27469deb-3df6-44b9-8b83-1fc5429c5a69-0' usage_metadata={'input_tokens': 242, 'output_tokens': 12, 'total_tokens': 254, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "user_question_2 = \"how many passengers survived\"\n",
    "initial_state = {\n",
    "\t\"csv_path\": \"train.csv\",  \n",
    "\t\"question\": user_question_2,\n",
    "\t\"attempts\": 0,\n",
    "\t\"data_database_name\": settings.database_name,\n",
    "\t\"memory_database_name\": \"memory_db\",\n",
    "\t\"table_name\": \"data_table\",\n",
    "\t\"sql_error\": False,\n",
    "\t\"analysis_goal\": \"\",\n",
    "\t\"max_analysts\": 3,\n",
    "\t\"human_analyst_feedback\": None,\n",
    "\t\"data_df\": {},\n",
    "\t\"ml_models\": {},\n",
    "\t\"visualizations\": {},\n",
    "\t\"database_schema\": None,\n",
    "\t\"analysis_summary\": None,\n",
    "\t\"memory_saver\": None,\n",
    "\t\"data_engine\": None,\n",
    "\t\"sql_query\": \"\",\n",
    "\t\"query_result\": \"\",\n",
    "\t\"query_rows\": \"\",\n",
    "\t\"relevance\": \"\"\n",
    "}\n",
    "\n",
    "result_2 = app.invoke(initial_state)\n",
    "print(\"Result:\", result_2[\"query_result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello, \n",
      "\n",
      "A total of 342 passengers survived.\n"
     ]
    }
   ],
   "source": [
    "query_result = result_2[\"query_result\"]\n",
    "\n",
    "# Extract only the \"content\" tuple\n",
    "content_tuple = next((item for item in query_result if item[0] == \"content\"), None)\n",
    "\n",
    "# Get the content text\n",
    "content_text = content_tuple[1] if content_tuple else \"No content found.\"\n",
    "\n",
    "formatted_output = f\"\\n{content_text}\"\n",
    "\n",
    "print(formatted_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
